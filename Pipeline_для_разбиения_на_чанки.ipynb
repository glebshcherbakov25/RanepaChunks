{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ba68538f",
      "metadata": {
        "id": "ba68538f"
      },
      "source": [
        "# Pipl для разбиения на чанки"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83e18b18",
      "metadata": {
        "id": "83e18b18"
      },
      "source": [
        "##   Импорт библиотек\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b45ecee0",
      "metadata": {
        "id": "b45ecee0"
      },
      "outputs": [],
      "source": [
        "import os  # работа с файловой системой\n",
        "import re  # регулярные выражения\n",
        "import math  # математические функции\n",
        "import requests  # HTTP-запросы\n",
        "from urllib.parse import unquote  # декодирование URL\n",
        "from pathlib import Path  # работа с путями\n",
        "from zipfile import ZipFile  # работа с ZIP-архивами\n",
        "from hashlib import md5  # генерация MD5-хешей\n",
        "import pandas as pd  # работа с таблицами"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f6c6743",
      "metadata": {
        "id": "5f6c6743"
      },
      "source": [
        "##  Список ссылок"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd26250f-8cf6-4a21-af75-5e74461e7cb5",
      "metadata": {
        "id": "dd26250f-8cf6-4a21-af75-5e74461e7cb5"
      },
      "source": [
        "Создаем список ссылок на документы с сайта Ранха"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f1dd96",
      "metadata": {
        "id": "b9f1dd96"
      },
      "outputs": [],
      "source": [
        "\n",
        "document_links = [\n",
        "    \"https://www.ranepa.ru/upload/doc/prikazy-ranhigs/%D0%9F%D0%BE%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%9F%D1%80%D0%B0%D0%B2%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D1%81%D1%82%D0%B2%D0%B0%20%D0%A0%D0%A4%20%D0%BE%D1%82%2012%20%D0%BC%D0%B0%D1%8F%202012%20%D0%B3%20N%20473%20%D0%9E%D0%B1%20%D1%83%D1%82%D0%B2%D0%B5%D1%80%D0%B6%D0%B4%D0%B5%D0%BD%D0%B8%D0%B8%20%D1%83%D1%81%D1%82%D0%B0%D0%B2%D0%B0.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/accred/vip-19.03.2025.zip\",\n",
        "    \"https://www.ranepa.ru/upload/doc/accred/vip-accred-20.05.2025.zip\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/2025/Pravila_priema_bac_spec_2025.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/2025/PP-2025-pril-1.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/2025/min_ball_2025.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/2025/min_ball_2025_fill.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/2025/PP-2025-pril-2_fil.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/2025/Prilogenie_3.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/2025/Prilogenie_4_Osobennosti_VI.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/prikazy-ranhigs/Pravila_priema_RANHiGS_2021_reglament_inv_ED.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/prikazy-ranhigs/Pologenie_Priemnoi_komisii_2024.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/prikazy-ranhigs/Pologenie__exam_apell_komиссии.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/prikazy-ranhigs/Pologenie_vstupitelnoe_ispитание_2023.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/prikazy-ranhigs/Pravila_Appeal_2023.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/2025/273-zакон_2020.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/2025/FZ_Ob_obразовании_v_RF_priem.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/Federalный_zakon_19-FZ.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/2025/152-FZ.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/Ukaz_27.04.2023_N_307.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/2025/%D0%9F%D0%BE%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%9F%D1%80%D0%B0%D0%B2%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D1%81%D1%82%D0%B2%D0%B0%20%D0%A0%D0%A4%20%D0%BE%D1%82%2027.04.2024%20N%20555%20%D0%B2%20%D1%80%D0%B5%D0%B4.%20%D0%BE%D1%82%2007.04.2025.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/Pravila_mat_kapital.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/2025/Postanovlenie_89.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/Poryadok_priema.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/Prikaz_233_552_04.04.2023.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/prikazy-ranhigs/Perechen_VI_27.11.2024_820.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/2025/Prikaz_571.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/2025/Prikaz_620.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/2025/Prikaz_17_02_2025_N_107.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/243_02.03.2023.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/2025/Prikaz_876.pdf\",\n",
        "    \"https://www.ranepa.ru/upload/doc/pk/npa/2025/Prikaz_219.pdf\",\n",
        "]\n",
        "\n",
        "#создаем здесь словарик, в который добавим ссылки на документы (потом используем в итоговой табице с чанками)\n",
        "filename_to_url = {}\n",
        "for url in document_links:\n",
        "    fname = unquote(Path(url).name)\n",
        "    filename_to_url[fname] = url\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4355b674",
      "metadata": {
        "id": "4355b674"
      },
      "source": [
        "## Сохранение документов"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5449eea-3117-4ab7-93f2-c7d987d41090",
      "metadata": {
        "id": "f5449eea-3117-4ab7-93f2-c7d987d41090"
      },
      "source": [
        " Здесь скачиваем документы по ссылкам из списка, сохраняем их в директорию SAVE_DIR внутри юпитера."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7af915",
      "metadata": {
        "id": "7e7af915"
      },
      "outputs": [],
      "source": [
        "\n",
        "SAVE_DIR = Path(\"ranepa_documents\")\n",
        "SAVE_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "def download_all():\n",
        "\n",
        "    for url in document_links:\n",
        "        fname = unquote(Path(url).name)\n",
        "        path = SAVE_DIR / fname\n",
        "\n",
        "        # Пропускаем, если скачан ранее\n",
        "        if path.exists():\n",
        "            print(f\" Уже скачано: {fname}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            resp = requests.get(url, timeout=60)\n",
        "            resp.raise_for_status()\n",
        "            path.write_bytes(resp.content)\n",
        "            print(f\" Скачан: {fname}\")\n",
        "\n",
        "            # Если файл — ZIP, распакуем в папку с его именем (без расширения)\n",
        "            if fname.lower().endswith(\".zip\"):\n",
        "                extract_to = SAVE_DIR / path.stem\n",
        "                extract_to.mkdir(exist_ok=True)\n",
        "                with ZipFile(path, \"r\") as zf:\n",
        "                    zf.extractall(extract_to)\n",
        "                print(f\" Распакован: {fname} → {extract_to}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Не удалось скачать {fname}: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cffa1cf",
      "metadata": {
        "id": "6cffa1cf"
      },
      "source": [
        "##  Конвертация PDF / DOCX / HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bd007a9-701a-46ae-96db-6c08eb8c3fe5",
      "metadata": {
        "id": "3bd007a9-701a-46ae-96db-6c08eb8c3fe5"
      },
      "source": [
        "Здесь с помощью PyPDF2 извлекаем текст со всех страниц (страницы, которые не содержат текста, заменяем на пустые строки). Также с помощью python-dox извлекаем текст из DOCX-параграфов. В зависимости от суффикса файла возвращаем чистый текст (для каждого суффикса свой метод)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a776a0f",
      "metadata": {
        "id": "2a776a0f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document as DocxDocument\n",
        "\n",
        "\n",
        "def pdf_to_text(fp: Path) -> str:\n",
        "\n",
        "    reader = PdfReader(str(fp))\n",
        "    text_pages = [page.extract_text() or \"\" for page in reader.pages]\n",
        "    return \"\\n\".join(text_pages)\n",
        "\n",
        "\n",
        "def docx_to_text(fp: Path) -> str:\n",
        "\n",
        "    doc = DocxDocument(str(fp))\n",
        "    return \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "\n",
        "\n",
        "def plain_text(fp: Path) -> str:\n",
        "\n",
        "    suffix = fp.suffix.lower()\n",
        "    if suffix == \".pdf\":\n",
        "        return pdf_to_text(fp)\n",
        "    if suffix == \".docx\":\n",
        "        return docx_to_text(fp)\n",
        "    if suffix in {\".txt\", \".html\", \".htm\"}:\n",
        "        return fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    return \"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c19761",
      "metadata": {
        "id": "e1c19761"
      },
      "source": [
        "## Смысловое разбиение (TF‑IDF + кластеризация)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43eb9b9c-990a-4d0d-bfed-9b81c4b77afd",
      "metadata": {
        "id": "43eb9b9c-990a-4d0d-bfed-9b81c4b77afd"
      },
      "source": [
        "Здесь прописываем функции, при помощи которых будем разбивать на чанки:\n",
        "- Разбиваем текст на предложения (сплит при помощи точек, вопросительных/воскрицательных знаков и пробелов\n",
        "- Векторизуем полученные предложения при помощи TfidfVectorizer(max_features=5000):\n",
        "- Находим нулевые векторы (если в предложении есть стоп слова или токены не вошли в словарь):\n",
        "    - Если все векторы оказываются нулевыми - просто возвращаем один чанк, состоящий из всего текста.\n",
        "    - Иначе кластеризуем только ненулевые векторы, используя агломеративную кластеризацию. Составляем список меток: для нулевых векторов метка 1, для остальных в зависимости от кластеризации. Проходимся по предложениям и склеиваем соседние, у которых метка совпадает"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf4d972",
      "metadata": {
        "id": "bdf4d972"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "def split_sentences(text: str):\n",
        "\n",
        "    parts = re.split(r\"(?<=[.!?])\\s+\", text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "\n",
        "def semantic_chunks(sentences, max_sent_per_chunk=5):\n",
        "\n",
        "    if not sentences:\n",
        "        return []\n",
        "\n",
        "    # TF‑IDF\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    mat = vectorizer.fit_transform(sentences)\n",
        "    dense = mat.toarray()  # shape = (n_sentences, n_features)\n",
        "\n",
        "    # Определяем, какие строки — полностью нулевые\n",
        "    zero_idx = [i for i, row in enumerate(dense) if not row.any()]\n",
        "    nonzero_idx = [i for i in range(len(sentences)) if i not in zero_idx]\n",
        "\n",
        "    # Если все предложения — «нулевые» → возвращаем один чанк (join)\n",
        "    if len(nonzero_idx) == 0:\n",
        "        return [\" \".join(sentences)]\n",
        "\n",
        "    # Определяем число кластеров\n",
        "    n_clusters = max(1, math.ceil(len(nonzero_idx) / max_sent_per_chunk))\n",
        "\n",
        "    if n_clusters >= len(nonzero_idx):\n",
        "        return [\" \".join(sentences)]\n",
        "\n",
        "    #Кластеризуем только ненулевые\n",
        "    sub_mat = dense[nonzero_idx]\n",
        "    model = AgglomerativeClustering(\n",
        "        n_clusters=n_clusters,\n",
        "        affinity=\"cosine\",\n",
        "        linkage=\"average\"\n",
        "    ).fit(sub_mat)\n",
        "    sub_labels = model.labels_\n",
        "\n",
        "    #Восстанавливаем полный список меток длины len(sentences):\n",
        "    labels = [-1] * len(sentences)\n",
        "    for idx, lab in zip(nonzero_idx, sub_labels):\n",
        "        labels[idx] = lab\n",
        "\n",
        "    #Склеиваем предложения с одинаковой меткой\n",
        "    chunks = []\n",
        "    buf = [sentences[0]]\n",
        "    cur_label = labels[0]\n",
        "    for sent, lab in zip(sentences[1:], labels[1:]):\n",
        "        if lab == cur_label:\n",
        "            buf.append(sent)\n",
        "        else:\n",
        "            chunks.append(\" \".join(buf))\n",
        "            buf = [sent]\n",
        "            cur_label = lab\n",
        "    if buf:\n",
        "        chunks.append(\" \".join(buf))\n",
        "\n",
        "    return chunks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430b7825",
      "metadata": {
        "id": "430b7825"
      },
      "source": [
        "## Наш пайплайн"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d665b0ae-527f-4e35-8c1f-62328a6b0d70",
      "metadata": {
        "id": "d665b0ae-527f-4e35-8c1f-62328a6b0d70"
      },
      "source": [
        "Собираем пайплайн, получаем итоговый датафрейм:\n",
        "- Проходим по всем файлам в сохраненной директории.\n",
        "    - Для каждого файла:\n",
        "         - извлекаем текст,\n",
        "         - разбиваем текст на предложения,\n",
        "         - собираем чанки,\n",
        "         - чистим чанки (удаляем слишком маленькие, дупликаты, убираем номера страниц и ненужные символы)\n",
        "         - для каждого чанкa готовим словарь со столбцами:\n",
        "           {\n",
        "             \"doc_id\":   первые 8 символов MD5 от fp.name,\n",
        "             \"chunk_id\": порядковый номер чанка в этом документе,\n",
        "             \"title\":    название документа,\n",
        "             \"text\":     текст чанка,\n",
        "             \"url\":      ссылка на документ,\n",
        "             \"doc_type\": \"Нормативный акт\",\n",
        "             \"length\":   количество символов\n",
        "           }\n",
        "    - Собираем полученные словари в list `rows`.\n",
        "    - Конвертируем rows в датафрейм и сохраняем в CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12bdec36",
      "metadata": {
        "id": "12bdec36",
        "outputId": "b5aee3c6-5d40-4269-9bde-44f073a8a5d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: 152-FZ.pdf → 547 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: 273-zakon_2020.pdf → 3101 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: Federalniy_zakon_19-FZ.pdf → 188 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: FZ_Ob_obrazovanii_v_RF_priem.pdf → 292 чанков (до фильтрации длиной)\n",
            "    Обработан: min_ball_2025.pdf → 1 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: min_ball_2025_fill.pdf → 1 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n",
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: Pologenie_Priemnoi_komisii_2024.pdf → 77 чанков (до фильтрации длиной)\n",
            "    Обработан: Pologenie_vstupitelnoe_ispitanie_2023.pdf → 23 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: Pologenie__exam_apell_komisii.pdf → 56 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: Postanovlenie_89.pdf → 156 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: PP-2025-pril-1.pdf → 88 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: PP-2025-pril-2_fil.pdf → 229 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: Pravila_Appeal_2023.pdf → 33 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: Pravila_mat_kapital.pdf → 82 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: Pravila_priema_bac_spec_2025.pdf → 226 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: Pravila_priema_RANHiGS_2021_reglament_inv_ED.pdf → 50 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Обработан: Prikaz_17_02_2025_N_107.pdf → 3099 чанков (до фильтрации длиной)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gserb\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def pipeline(limit=None):\n",
        "\n",
        "    rows = []\n",
        "    files = list(SAVE_DIR.rglob(\"*\"))\n",
        "    if limit:\n",
        "        files = files[:limit]\n",
        "\n",
        "    for fp in files:\n",
        "        if not fp.is_file():\n",
        "            continue\n",
        "\n",
        "        text_raw = plain_text(fp)\n",
        "        if not text_raw.strip():\n",
        "\n",
        "            continue\n",
        "\n",
        "        sentences = split_sentences(text_raw)\n",
        "        if not sentences:\n",
        "            continue\n",
        "\n",
        "        # Получаем чанки\n",
        "        chunks = semantic_chunks(sentences, max_sent_per_chunk=5)\n",
        "        # Генерируем уникальный doc_id\n",
        "        doc_id = md5(fp.name.encode()).hexdigest()[:8]\n",
        "        # Читаемый заголовок\n",
        "        title = fp.stem.replace(\"_\", \" \")\n",
        "        # Получаем ссылку из словаря со второй ячейки\n",
        "        url = filename_to_url.get(fp.name, \"\")\n",
        "        # \"Нормативный акт :D)\n",
        "        doc_type = \"Нормативный акт\"\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            # Чистим, чистим и еще раз чистим\n",
        "            lines = chunk.splitlines()\n",
        "            cleaned_lines = []\n",
        "            for line in lines:\n",
        "                s = line.strip()\n",
        "                if re.fullmatch(r\"\\d+\", s):\n",
        "                    continue\n",
        "                if re.fullmatch(r\"(Стр\\.?|Страница)\\s*\\d+\", s, flags=re.IGNORECASE):\n",
        "                    continue\n",
        "                cleaned_lines.append(s)\n",
        "            text_clean = \" \".join(cleaned_lines)\n",
        "\n",
        "            # Скипаем, если после чистки мало символов\n",
        "            if len(text_clean) < 100:\n",
        "                continue\n",
        "\n",
        "            rows.append({\n",
        "                \"doc_id\":   doc_id,\n",
        "                \"chunk_id\": i,\n",
        "                \"title\":    title,\n",
        "                \"text\":     text_clean,\n",
        "                \"url\":      url,\n",
        "                \"doc_type\": doc_type,\n",
        "                \"length\":   len(text_clean)\n",
        "            })\n",
        "\n",
        "        print(f\"    Обработан: {fp.name} → {len(chunks)} чанков (до фильтрации длиной)\")\n",
        "\n",
        "    # Формируем DataFrame и сохраняем CSV\n",
        "    df = pd.DataFrame(rows)\n",
        "    OUT_DIR = Path(\"data/processed\")\n",
        "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    out_csv = OUT_DIR / \"ranepa_semantic_chunks_full.csv\"\n",
        "    df.to_csv(out_csv, index=False, sep=\";\", quotechar='\"', encoding=\"utf-8-sig\")\n",
        "    print(f\"\\n Готово: {out_csv}  ({len(df)} строк всего)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline(limit=None)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}